# BLRQ-BV
（1）For English, we adopt the bert-base-uncased model. For Chinese, we use the bert-base-chinese model. 
（2）Meanwhile, different tokenizers are employed for the BiLSTM model in Chinese and English scenarios respectively.
（3）unified config.py is the configuration file, and readers need to modify it according to the input directory of the actual dataset. Meanwhile, readers need to specify the output directories of the process file and the result file according to the actual path.
